import numpy as np
import random
import matplotlib.pyplot as plt
import pygame
import time
from collections import defaultdict

class WarehouseEnvCNN:
    def __init__(self, grid_size=10, obstacle_ratio=0.1):
        self.grid_size = grid_size
        self.obstacle_ratio = obstacle_ratio
        self.actions = ['up', 'down', 'left', 'right']
        self.action_dict = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}
        self.reset()

    from collections import deque

    def reset(self):
        while True:
            self.obstacles = set()
            self.visit_count = defaultdict(int)
            self.grid = np.zeros((3, self.grid_size, self.grid_size), dtype=np.float32)
            self.agent_pos = self._random_empty_cell()
            self.pickup_pos = self._random_empty_cell()
            self.drop_pos = self._random_empty_cell()
            self.return_pos = self._random_empty_cell()
            self.has_item = False
            self.delivered = False
            self.done = False
            self.prev_pos = None
            self.prev_prev_pos = None

            total_cells = self.grid_size ** 2
            num_obstacles = int(total_cells * self.obstacle_ratio)
            while len(self.obstacles) < num_obstacles:
                ox, oy = random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1)
                if (ox, oy) not in [self.agent_pos, self.pickup_pos, self.drop_pos, self.return_pos]:
                    self.obstacles.add((ox, oy))
            for (ox, oy) in self.obstacles:
                self.grid[2, ox, oy] = 1.0

        # ğŸ§  ìœ íš¨ì„± ê²€ì‚¬: agent â†’ pickup â†’ drop â†’ return ê°€ëŠ¥í•´ì•¼ í•¨
            if self._is_reachable(self.agent_pos, self.pickup_pos) and \
            self._is_reachable(self.pickup_pos, self.drop_pos) and \
            self._is_reachable(self.drop_pos, self.return_pos):
                break  # ë„ë‹¬ ê°€ëŠ¥í•˜ë©´ ë§µ í™•ì •

        return self._get_state()
    def _is_reachable(self, start, goal):
        visited = set()
        queue = deque([start])
        while queue:
            x, y = queue.popleft()
            if (x, y) == goal:
                return True
            for dx, dy in self.action_dict.values():
                nx, ny = x + dx, y + dy
                if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:
                    if (nx, ny) not in self.obstacles and (nx, ny) not in visited:
                        visited.add((nx, ny))
                        queue.append((nx, ny))
        return False

    def _random_empty_cell(self):
        while True:
            pos = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))
            if pos not in self.obstacles:
                return pos

    def _get_state(self):
        self.grid[0].fill(0)  # agent
        self.grid[1].fill(0)  # goal
        ax, ay = self.agent_pos
        self.grid[0, ax, ay] = 1.0
        if not self.has_item:
            gx, gy = self.pickup_pos
        elif not self.delivered:
            gx, gy = self.drop_pos
        else:
            gx, gy = self.return_pos
        self.grid[1, gx, gy] = 1.0
        return np.copy(self.grid)

    def step(self, action):
        dx, dy = self.action_dict[action]
        x, y = self.agent_pos
        nx, ny = x + dx, y + dy
        reward = -1  # ê¸°ë³¸ ì´ë™ í˜ë„í‹°

        # ----- ì´ë™ ì²˜ë¦¬ -----
        if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:
            if (nx, ny) not in self.obstacles:
                self.agent_pos = (nx, ny)
            else:
                reward = -10  # ì¥ì• ë¬¼ ì¶©ëŒ
        else:
            reward = -10 # ë²½ ì¶©ëŒ

        # ----- ì¤‘ë³µ ë°©ë¬¸ íŒ¨ë„í‹° -----
        self.visit_count[self.agent_pos] += 1
        if self.visit_count[self.agent_pos] > 1:
            reward -= 1 * (self.visit_count[self.agent_pos] - 1)  # ëˆ„ì  ê°ì 

        # ----- ëª©í‘œ ì§€ì ê¹Œì§€ì˜ shaping reward -----
        gx, gy = (
            self.pickup_pos if not self.has_item
            else self.drop_pos if not self.delivered
            else self.return_pos
        )
        ax, ay = self.agent_pos
        curr_dist = abs(ax - gx) + abs(ay - gy)

        # ğŸ”¸ min_distê°€ ì—†ë‹¤ë©´ ì´ˆê¸°í™”
        if not hasattr(self, 'min_dist') or self.min_dist is None:
            self.min_dist = curr_dist

        # âœ… 1. ìƒˆë¡œ ë” ê°€ê¹Œì›Œì¡Œì„ ë•Œë§Œ ë³´ìƒ (ì¤‘ë³µ ì—†ì´)
        if curr_dist < self.min_dist:
            delta = self.min_dist - curr_dist
            reward += 2 ** delta         # ğŸ ì§€ìˆ˜ì  ë³´ìƒ
            self.min_dist = curr_dist    # ìµœì†Œ ê±°ë¦¬ ê°±ì‹ 

        # âœ… 2. ì´ì „ ìœ„ì¹˜ë³´ë‹¤ ê°€ê¹Œì›Œì¡Œì„ ë•Œë§Œ ì†ŒëŸ‰ shaping ë³´ìƒ
        if self.prev_pos:
            prev_dist = abs(self.prev_pos[0] - gx) + abs(self.prev_pos[1] - gy)
            if curr_dist < prev_dist:
                reward += 2.0  # ì†ŒëŸ‰ shaping reward

        # âœ… 3. ëª©í‘œ ì§€ì  ë„ë‹¬ ì‹œ ê±°ë¦¬ ê¸°ë¡ ì´ˆê¸°í™”
        if self.agent_pos == (gx, gy):
            self.min_dist = None       

        # ----- ì´ë²¤íŠ¸ ë„ë‹¬ ë³´ìƒ -----
        if self.agent_pos == self.pickup_pos and not self.has_item:
            self.has_item = True
            reward += 100
        elif self.agent_pos == self.drop_pos and self.has_item and not self.delivered:
            self.delivered = True
            self.has_item = False
            reward += 60
        elif self.agent_pos == self.return_pos and self.delivered:
            self.done = True
            reward += 20
        if self.visit_count[self.agent_pos] > 1:
            reward -= 5
        # ----- ì œìë¦¬ ë° ì™”ë‹¤ ê°”ë‹¤ íŒ¨ë„í‹° -----
        if self.prev_pos == self.agent_pos:
            reward -= 20  # ì œìë¦¬ í–‰ë™ ê°ì 
        if hasattr(self, 'prev_prev_pos') and self.prev_prev_pos == self.agent_pos:
            reward -= 15  # ë‘ ì¹¸ ì „ ìœ„ì¹˜ë¡œ ë˜ëŒì•„ì˜¨ ê²½ìš° ê°ì 

        # ----- ìœ„ì¹˜ ê¸°ë¡ -----
        self.prev_prev_pos = self.prev_pos
        self.prev_pos = self.agent_pos

        # reward í•˜í•œì„  ì ìš©
        reward = max(reward, -50)  # ì˜ˆ: rewardëŠ” ìµœì†Œ -50ê¹Œì§€ë§Œ

        return self._get_state(), reward, self.done
    
import torch
import torch.nn as nn

class CNNQNetwork(nn.Module):
    def __init__(self, input_channels=3, action_size=4, grid_size=3):
        super(CNNQNetwork, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )
        self.fc = nn.Sequential(
            nn.Linear(64 * grid_size * grid_size, 256),  # ë™ì ìœ¼ë¡œ ì²˜ë¦¬
            nn.ReLU(),
            nn.Linear(256, action_size)
        )

    def forward(self, x):
        x = self.conv(x)
        return self.fc(x)

import torch.optim as optim
from collections import deque
import random

def train_dqn(env, episodes=1000, batch_size=32, gamma=0.95, epsilon=1.0,
              epsilon_min=0.1, epsilon_decay=0.997, lr=0.001, buffer_size=10000,max_steps=50):

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    q_net = CNNQNetwork(grid_size=env.grid_size).to(device)
    target_net = CNNQNetwork(grid_size=env.grid_size).to(device)
    target_net.load_state_dict(q_net.state_dict())
    target_net.eval()

    success_list = []

    optimizer = optim.Adam(q_net.parameters(), lr=lr)
    memory = deque(maxlen=buffer_size)
    action_list = ['up', 'down', 'left', 'right']

    for ep in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0

        for _ in range(max_steps):
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)  # (1, 3, 10, 10)

            if random.random() < epsilon:
                action_idx = random.randint(0, 3)
            else:
                with torch.no_grad():
                    q_vals = q_net(state_tensor)  # shape: (1, 4)
                    q_vals += torch.rand_like(q_vals) * 1e-2  # tie-breaker (optional)
                    action_idx = torch.argmax(q_vals[0]).item()  # âœ… ì•ˆì „í•˜ê³  í™•ì‹¤í•¨


            next_state, reward, done = env.step(action_list[action_idx])
            memory.append((state, action_idx, reward, next_state, done))
            total_reward += reward
            state = next_state

            if len(memory) >= batch_size:
                batch = random.sample(memory, batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = torch.FloatTensor(states).to(device)
                next_states = torch.FloatTensor(next_states).to(device)
                actions = torch.LongTensor(actions).unsqueeze(1).to(device)
                rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)
                dones = torch.BoolTensor(dones).unsqueeze(1).to(device)

                q_vals = q_net(states).gather(1, actions)
                with torch.no_grad():
                    next_q = target_net(next_states).max(1, keepdim=True)[0]
                    target = rewards + gamma * next_q * (~dones)

                loss = nn.MSELoss()(q_vals, target)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            if done:
                break

        if ep % 10 == 0:
            target_net.load_state_dict(q_net.state_dict())
            # ----- ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ -----
            if env.has_item:
                success_list.append(1)
                success = True
            else:
                success_list.append(0)
                success = False

            cumulative_success_rate = sum(success_list) / len(success_list)

            # ----- ë¡œê·¸ ì¶œë ¥ -----
            print(f"ğŸ“˜ Episode {ep}, Total Reward: {total_reward:.2f}, "
                f"Epsilon: {epsilon:.3f}, "
                f"âœ… Success: {success}, "
                f"ğŸ¯ Success Rate: {cumulative_success_rate:.3f}")

        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

    print("âœ… í•™ìŠµ ì™„ë£Œ")
    torch.save(q_net.state_dict(), "/Users/jeon-yonghyeon/Desktop/hanyang/3-1/aix/ê³¼ì œ/enw/dqn_model.pth")
    print("ğŸ’¾ ì •ì±… ì €ì¥ë¨: dqn_cnn_policy.pth")
    return q_net

def test_policy(env, q_net, episodes=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    q_net.eval()
    action_list = ['up', 'down', 'left', 'right']

    for ep in range(episodes):
        state = env.reset()
        done = False
        step = 0
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ {ep + 1}")
        while not done and step < 100:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                q_vals = q_net(state_tensor)
                action_idx = torch.argmax(q_vals).item()
            state, reward, done = env.step(action_list[action_idx])
            print(f"Step {step:2d}, Action: {action_list[action_idx]}, Reward: {reward}")
            step += 1



CELL_SIZE = 100
WINDOW_SIZE = 500  # 5x5 ë§µì´ë©´ 500x500, 10x10 ë§µì´ë©´ 1000x1000 ë“±ìœ¼ë¡œ ì¡°ì ˆ

def draw_env(state, grid_size):
    pygame.init()
    screen = pygame.display.set_mode((CELL_SIZE * grid_size, CELL_SIZE * grid_size))
    pygame.display.set_caption("DQN Agent View")
    screen.fill((255, 255, 255))

    for i in range(grid_size):
        for j in range(grid_size):
            x = j * CELL_SIZE
            y = i * CELL_SIZE
            rect = pygame.Rect(x, y, CELL_SIZE, CELL_SIZE)
            pygame.draw.rect(screen, (200, 200, 200), rect, 1)

            agent = state[0, i, j]
            goal = state[1, i, j]
            obstacle = state[2, i, j]

            if obstacle == 1:
                pygame.draw.rect(screen, (50, 50, 50), rect)
            elif goal == 1:
                pygame.draw.rect(screen, (255, 0, 0), rect)
            elif agent == 1:
                pygame.draw.rect(screen, (0, 0, 255), rect)

    pygame.display.flip()

def run_with_gui(env, q_net):
    pygame.init() 
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    q_net.eval()
    state = env.reset()
    done = False
    step = 0
    clock = pygame.time.Clock()

    while not done and step < 100:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                return

        draw_env(state, env.grid_size)
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_vals = q_net(state_tensor)
            action_idx = torch.argmax(q_vals).item()

        state, reward, done = env.step(env.actions[action_idx])
        print(f"Step {step}, Action: {env.actions[action_idx]}, Reward: {reward}")
        time.sleep(0.5)
        step += 1

    time.sleep(2)
    pygame.quit()


import os
if __name__ == "__main__":
    env = WarehouseEnvCNN(grid_size=5, obstacle_ratio=0.1)
    print("ğŸš€ í•™ìŠµ ì‹œì‘")
    model = train_dqn(env, episodes=1000, epsilon_min=0.05)
    
    print("ğŸ¯ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    run_with_gui(env, model)  # í…ŒìŠ¤íŠ¸ë¥¼ pygame GUIë¡œ ì§„í–‰ 